{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5c5883",
   "metadata": {},
   "source": [
    "# Питон и машинное обучение\n",
    "\n",
    "# Модуль 9a. Метод поиска ближайших соседей\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c89e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (14,9)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e523f6b",
   "metadata": {},
   "source": [
    "## Классификация объектов по бесконечному количеству классов методом поиска ближайших соседей\n",
    "\n",
    "Задача классификации по бесконечному количеству классов возникает в следующих случаях:\n",
    "- подбор похожих объектов по образцу:\n",
    "    - рекомендательные системы\n",
    "    - поиск фото по текстовому описанию и по фото\n",
    "    - поиск и идентификация аномалий на рентгеновских снимках/МРТ/КТ\n",
    "- идентификация объектов по различным паттернам:\n",
    "    - поиск людей по фото, деталям фигуры, походки, прочее\n",
    "    - поиск музыки по фрагментам\n",
    "    - \"антиплагиат\"\n",
    "- обнаружение новых трендов и тем для обсуждения в социальных медиа и в интернете\n",
    "\n",
    "Обычно эта задача решается при помощи алгоритма kNN - поиска $k$ ближайших соседей. Некоторые относят данный алгоритм в раздел \"Обучение с учителем\", но на самом деле, при каждом прогнозе данный алгоритм использует всю обучающую выборку. И поэтому вообще сомнительно утверждать то, что это имеет отношение к машинному обучению. Скорее всего, это дальнейшее развитие темы кластеризации данных.\n",
    "\n",
    "Для алгоритма k-ближайших соседей (kNN) важен следующий препроцессинг данных:\n",
    "\n",
    "1. **Обработка пропущенных значений**: Заполните или удалите пропущенные значения, чтобы алгоритм мог корректно вычислять расстояния.\n",
    "2. **Преобразование категориальных данных**: Конвертируйте категориальные признаки в числовые для возможности вычисления расстояний.\n",
    "3. **Удаление выбросов**: Избавьтесь от аномальных значений, которые могут исказить результаты.\n",
    "3. **Масштабирование признаков**: Унифицируйте масштаб всех признаков, чтобы один не доминировал над другими. Можно использовать ```MinMaxScaler()```, ```StandardScaler()``` или ```normalization()```.\n",
    "5. **Уменьшение размерности**: Сократите количество признаков, чтобы избежать проблемы \"проклятия размерности\".\n",
    "\n",
    "Также на качество работы kNN влияет __выбор метрики расстояния__ (Евклидово, Косинусное, Хэмминга, пр.). Вот наиболее часто используемые метрики:\n",
    "\n",
    "1. **Евклидово расстояние**: Идеально подходит для непрерывных числовых данных с одинаковым масштабом. \n",
    "\n",
    "2. **Расстояние Хэмминга**: Расстояние Хэмминга подсчитывает число позиций, в которых соответствующие символы двух строк различны. Хорошо работает с категориальными или бинарными данными. Пример: поиск изображений по хэш-значениям.\n",
    "\n",
    "3. **Косинусное сходство**: Косинусное сходство измеряет угол между двумя векторами в многомерном пространстве, что позволяет судить о схожести по направлению, а не по магнитуде векторов. Пример: анализ текстов, где данные представлены в виде векторов слов или фраз.\n",
    "\n",
    "\n",
    "Рассмотрим на примере датасета \"Рукописные цифры\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "ix_random_image = 15\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.imshow(X[ix_random_image].reshape((8, 8)), cmap='gray_r')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Source image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=10,\n",
    "                             algorithm='brute',\n",
    "                             metric='euclidean')\n",
    "\n",
    "neighbors.fit(X)\n",
    "\n",
    "distances, indices = neighbors.kneighbors(X[ [ix_random_image] ])\n",
    "print(distances)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 2))\n",
    "for i, (idx, digit) in enumerate(zip(indices[0], X[ indices[0] ])):\n",
    "    ax = fig.add_subplot(1, 10, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(digit.reshape((8, 8)), cmap='gray_r')\n",
    "    ax.set_title(f\"idx={idx}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c0d4f",
   "metadata": {},
   "source": [
    "Классификатор ```KNeighborsClassifier``` может предсказывать и класс, и вероятность отнесения к классу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e564435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size=0.2, \n",
    "                                                   random_state=20231110,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8405b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10,\n",
    "                             algorithm='brute',\n",
    "                             metric='euclidean')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy score on test set: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"ROC-AUC score on test set: {roc_auc_score(y_test, y_pred_proba, multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33300d39",
   "metadata": {},
   "source": [
    "#### ⁉️ Задание\n",
    "\n",
    "Преобразуйте данные в 2D методами главных компонент и t-SNE. Выполните кросс-валидацию классификатора ```KNeighborsClassifier``` на этих множествах для метрик ```accuracy_score``` и ```roc_auc```. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
