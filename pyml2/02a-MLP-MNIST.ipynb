{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938ee5fa",
   "metadata": {},
   "source": [
    "# Python и машинное обучение: нейронные сети и компьютерное зрение\n",
    "\n",
    "## Модуль 2A. Полносвязная нейросеть\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b429c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8358bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "    \"mps\" if torch.backends.mps.is_built() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "print(X.max(), X.min())\n",
    "print(y[:12])\n",
    "print(np.unique(y))\n",
    "print(np.unique(y).shape)\n",
    "print(X[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tn = torch.tensor(X * 1.0/(X.max()-X.min()), \n",
    "                    device=device, \n",
    "                    dtype=torch.float32\n",
    "                   )\n",
    "y_tn = torch.tensor(y, device=device)\n",
    "X_tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tn, y_tn, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=20231110,\n",
    "                                                   stratify = y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_train.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4a28b",
   "metadata": {},
   "source": [
    "Сделаем класс для модели с возможностью вариативности по следующим параметрам:\n",
    "- кол-во нейронов в скрытом слое\n",
    "- функции активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDigits_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8*8, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vary = 'activation' #здесь пишем, что варьируем\n",
    "var_values = ['relu', 'tanh', 'sigmoid'] # здесь перечисляем варианты\n",
    "\n",
    "\n",
    "dict_vary = {'hidden': 52,\n",
    "            'activation': 'sigmoid',\n",
    "            'lr': 0.2,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'SGD',\n",
    "            'epochs': 100}\n",
    "\n",
    "dict_acc = {} # here we collect data for comparison\n",
    "\n",
    "for var in var_values:\n",
    "    print(f\"{vary}: {var}\")\n",
    "    dict_vary[vary] = var\n",
    "    model = MLPDigits_vary( **dict_vary ).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \\\n",
    "            if dict_vary['optimizer']=='RMSprop' else \\\n",
    "                torch.optim.Adam(model.parameters(), lr=dict_vary['lr']) \\\n",
    "            if dict_vary['optimizer']=='Adam' else \\\n",
    "                    torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum'])\n",
    "    results = train(model, epochs=dict_vary['epochs'], report_positions=10)\n",
    "    dict_acc[f'err_{var}'] = results['val_loss']\n",
    "    dict_acc[f'acc_{var}'] = results['val_acc']\n",
    "    dict_acc[f'epochs_{var}'] = results['epoch_count']\n",
    "    \n",
    "    plot_results(results)\n",
    "\n",
    "    \n",
    "fig_var, axs_var = plt.subplots(1,2)\n",
    "fig_var.set_size_inches(10,3)\n",
    "for var in var_values:\n",
    "    axs_var[0].plot(dict_acc[f'epochs_{var}'], dict_acc[f'err_{var}'], label=f'loss on {vary}={var}')\n",
    "    axs_var[0].legend()\n",
    "    axs_var[1].plot(dict_acc[f'epochs_{var}'], dict_acc[f'acc_{var}'], label=f'acc on {vary}={var}')\n",
    "    axs_var[1].legend()\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac1320",
   "metadata": {},
   "source": [
    "#### Практика\n",
    "\n",
    "Используя код выше, поэкспериментируйте с моделью:\n",
    " - попробуйте изменить количество весов в скрытом и выходном слоях (попробуйте 50, 32, 16, 8, 2, 1);\n",
    " - попробуйте различную скорость обучения;\n",
    " - попробуйте различные функции активации;\n",
    " - поэкпериметируйте с количеством эпох.\n",
    " \n",
    "Постарайтесь выйти на оптимальный уровень точности и скорости обучения.\n",
    " \n",
    "Сообщите максимальное значение точности на валидационном подмножестве данного датасета, конфигурацию модели и параметры обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_vary = ? # ваши параметры настройки модели\n",
    "\n",
    "model = MLPDigits_vary( **dict_vary ).to(device)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \\\n",
    "        if dict_vary['optimizer']=='RMSprop' else \\\n",
    "            torch.optim.Adam(model.parameters(), lr=dict_vary['lr']) \\\n",
    "        if dict_vary['optimizer']=='Adam' else \\\n",
    "                torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum'])\n",
    "results = train(model, epochs=dict_vary['epochs'], report_positions=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b31951",
   "metadata": {},
   "source": [
    "### Загрузка данных в модель в виде мини-батчей\n",
    "\n",
    "Выше мы обучали модель на всех имеющихся данных. Но при обучении нейросетей данные чаще всего загружают в виде мини-пакетов (мини-батчей). Это делается для того, чтобы:\n",
    "- обучать модель на разнообразии данных: в рамках одной эпохи каждый цикл обучения она будет видеть уникальные данные;\n",
    "- минимизировать объем памяти, используемой при обучении;\n",
    "- ускорить работу оптимизатора: он будет гораздо быстрее обрабатывать небольшой объем данных6 нежели чем весь датасет.\n",
    "\n",
    "Для этого можно использовать встроенный в PyTorch механизм генерации мини-батчей. На базе стандартного генератора PyTorch можно создать свой собственный, который будет отправлять в модель именно ваши данные в нужном именно вам виде.\n",
    "\n",
    "Для начала нужно создать свой класс на базе класса PyTorch ```Dataset```:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "````\n",
    "\n",
    "Затем на базе этого класса будут созданы объекты-генераторы данных, уже на этапе создания в них будет передана функция ```transforms.ToTensor()``` для преобразования изображений \"на лету\", по требованию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fbf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс генератора:\n",
    "class DatasetDigits(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # сразу представляем цифру как ndarray размерностью (Height * Width * Channels)\n",
    "        # конвертируем цифры в np.uint8 [Unsigned integer (0 to 255)] - стандарт для изображений\n",
    "        # чтобы отрабатывала стандартная функция ToTensor(), мы определяем размерность тензора (H, W, C)\n",
    "        image = self.X[index].astype(np.uint8).reshape((8, 8, 1))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (image, self.y[index])  \n",
    "        else:\n",
    "            return image\n",
    "\n",
    "# снова сделаем разбиение на обучающую и валидационную выборки\n",
    "# старые переменные X_train, X_val... - это нормированные тензоры, а нам нужны изображения в исходном формате\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=20231110,\n",
    "                                                   stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# создадим генераторы обучающих и тестовых данных:\n",
    "train_data = DatasetDigits(X_train, y_train, transform=transforms.ToTensor())\n",
    "val_data = DatasetDigits(X_val, y_val, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(list(range(y_train.shape[0]))))\n",
    "val_generator=  torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(list(range(y_val.shape[0]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(model, \n",
    "                  train_generator,\n",
    "                  valid_generator,\n",
    "                  batch_size=20, epochs=40, report_positions=20, **kwargs):\n",
    "    \n",
    "    results = {'epoch_count': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    # прогоняем данные по нейросети\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = valid_loss = 0.0; \n",
    "        train_correct = valid_correct = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_generator:\n",
    "            \n",
    "            X_batch = X_batch.to(device); y_batch = y_batch.to(device)\n",
    "            \n",
    "            y_logps = model(X_batch) #логарифмы вероятности отнесения к классам\n",
    "            loss = criterion(y_logps, y_batch) #кросс-энтропия\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "            train_correct += accuracy_fn(y_logps, y_batch) * y_batch.shape[0]\n",
    "            \n",
    "        train_loss /= len(train_generator.dataset)\n",
    "        train_acc = 100 * train_correct / len(train_generator.dataset)\n",
    "\n",
    "        # Валидацию тоже делаем по батчам\n",
    "        model.eval()         \n",
    "        \n",
    "        for valid_batches, (X_val_batch, y_val_batch) in enumerate(valid_generator):\n",
    "            X_val_batch = X_val_batch.to(device); y_val_batch = y_val_batch.to(device)\n",
    "            y_batch_logps = model(X_val_batch)\n",
    "            loss = criterion(y_batch_logps, y_val_batch)\n",
    "            \n",
    "            valid_loss += loss.data.item()\n",
    "            valid_correct += accuracy_fn(y_batch_logps, y_val_batch) * y_val_batch.shape[0]\n",
    "            \n",
    "        valid_loss /= len(valid_generator.dataset)\n",
    "        valid_acc = 100 * valid_correct / len(valid_generator.dataset)\n",
    "        \n",
    "        results['epoch_count'] += [epoch]\n",
    "        results['train_loss'] += [ train_loss ]\n",
    "        results['train_acc'] += [ train_acc ]\n",
    "        results['val_loss'] += [ valid_loss ]\n",
    "        results['val_acc'] += [ valid_acc ]\n",
    "        \n",
    "        if epoch % (epochs // report_positions) == 0 or epochs<50:\n",
    "            print(f\"Epoch: {epoch+1:4.0f} | Train Loss: {train_loss:.5f}, \"+\\\n",
    "                  f\"Accuracy: {train_acc:.2f}% | \\\n",
    "                Validation Loss: {valid_loss:.5f}, Accuracy: {valid_acc:.2f}%\")\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5bd43",
   "metadata": {},
   "source": [
    "Будем обучать ту же модель, что и в прошлый раз, ```MLPDigits_vary()```, только добавим ей \"выпрямляющий\" код на входе, чтобы она могла работать с тензорами-изображениями PyTorch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDigits_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8*8, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 8 * 8) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a4ea0",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с моделью, варьируйте количество эпох и размер пакета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDigits_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8*8, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 8 * 8) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vary = 'batch_size' #здесь пишем, что варьируем\n",
    "var_values = [64, 25, 10] # здесь перечисляем варианты\n",
    "\n",
    "\n",
    "dict_vary = {'hidden': 12,\n",
    "            'activation': 'tanh',\n",
    "            'batch_size': 20,\n",
    "            'lr': 0.002,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'RMSprop',\n",
    "            'epochs': 30}\n",
    "\n",
    "dict_acc = {} # here we collect data for comparison\n",
    "\n",
    "for var in var_values:\n",
    "    print(f\"{vary}: {var}\")\n",
    "    dict_vary[vary] = var\n",
    "    model = MLPDigits_vary( **dict_vary ).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \\\n",
    "            if dict_vary['optimizer']=='RMSprop' else \\\n",
    "                torch.optim.Adam(model.parameters(), lr=dict_vary['lr']) \\\n",
    "            if dict_vary['optimizer']=='Adam' else \\\n",
    "                    torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum'])\n",
    "    \n",
    "    # добавляем создание генератора для нужного нам количества батчей\n",
    "    train_generator = torch.utils.data.DataLoader(train_data, \n",
    "                                                  batch_size=dict_vary['batch_size'],\n",
    "        sampler=SubsetRandomSampler(list(range(y_train.shape[0]))))\n",
    "    valid_generator = torch.utils.data.DataLoader(val_data, \n",
    "                                                  batch_size=dict_vary['batch_size'],\n",
    "        sampler=SubsetRandomSampler(list(range(y_val.shape[0]))))\n",
    "    \n",
    "    results = train_batches(model, train_generator, valid_generator, report_positions=10, **dict_vary)\n",
    "    dict_acc[f'err_{var}'] = results['val_loss']\n",
    "    dict_acc[f'acc_{var}'] = results['val_acc']\n",
    "    dict_acc[f'epochs_{var}'] = results['epoch_count']\n",
    "    \n",
    "    plot_results(results)\n",
    "\n",
    "    \n",
    "fig_var, axs_var = plt.subplots(1,2)\n",
    "fig_var.set_size_inches(10,3)\n",
    "for var in var_values:\n",
    "    axs_var[0].plot(dict_acc[f'epochs_{var}'], dict_acc[f'err_{var}'], label=f'loss on {vary}={var}')\n",
    "    axs_var[0].legend()\n",
    "    axs_var[1].plot(dict_acc[f'epochs_{var}'], dict_acc[f'acc_{var}'], label=f'acc on {vary}={var}')\n",
    "    axs_var[1].legend()\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "summary(model, \n",
    "        input_size=X_train.shape, \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        device=device\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ef746",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Попробуем теперь обучить нашу модель на датасете MNIST, это те же \"рукописные цифры\", но в разрешении 28x28 и в количестве 50000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad733823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transforms.ToTensor())\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры нормализации\n",
    "imgs = torch.stack([img for img, _ in trainset], dim=0)\n",
    "\n",
    "mean = imgs.view(1, -1).mean(dim=1)    # or imgs.mean()\n",
    "std = imgs.view(1, -1).std(dim=1)     # or imgs.std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f10c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=mnist_transforms)\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34244e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_generator_MNIST = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_generator_MNIST = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874272fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем батч\n",
    "dataiter = iter(train_generator_MNIST)\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d1bfc",
   "metadata": {},
   "source": [
    "Cоздадим модель на базе последней, повысим размерность входного слоя до 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90510864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3203f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vary = {'hidden': 512,\n",
    "            'activation': 'tanh',\n",
    "            'batch_size': batch_size,\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'RMSprop',\n",
    "            'epochs': 10}\n",
    "\n",
    "model = MLP_MNIST_vary( **dict_vary ).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "results = train_batches(model, \n",
    "                        train_generator_MNIST, \n",
    "                        val_generator_MNIST, report_positions=10, **dict_vary)\n",
    "\n",
    "plot_results(results)\n",
    "summary(model, \n",
    "        input_size=images.shape, \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        device=device\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521306c",
   "metadata": {},
   "source": [
    "## Регуляризация добавлением слоя Dropout\n",
    "\n",
    "Добавим в модель \"прореживание\" - слой dropout, который в момент обучения блокирует ряд нейронов (позиций в матрицах соотв. слоев)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a8625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
